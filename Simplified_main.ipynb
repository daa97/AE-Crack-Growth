{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138316ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, umap\n",
    "import scipy.stats, scipy.signal\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entropy Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PMF(waveform):                  # probability mass function of waveform\n",
    "    hist = histogram(waveform)      # histogram of waveform\n",
    "    return hist / sum(hist)         # normalize to sum of 1\n",
    "    \n",
    "def histogram(wave):\n",
    "    \"Returns a histogram of values with bin widths determined by the Freedman-Diaconis rule.\"\n",
    "    n = wave.shape[0]                               # number of points in time series\n",
    "    data_range = np.max(wave) - np.min(wave)        # range of voltage values\n",
    "    bin_width = 2 * IQR(wave) / (n ** (1 / 3))      # width of each bin\n",
    "    num_bins = math.ceil(data_range / bin_width)    # number of bins\n",
    "    edges = bin_width*np.arange(num_bins+1)         # bin boundaries\n",
    "    # return number of points in each bin\n",
    "    return np.array([np.count_nonzero((edges[i]<=wave) & (wave<edges[i+1])) for i in range(num_bins)])\n",
    "\n",
    "IQR = lambda X: np.quantile(X,.75) - np.quantile(X,.25)\n",
    "outlier = lambda X: X>(np.quantile(X,.75) +IQR(X))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hit Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hit:\n",
    "    \"represents a waveform with a fixed number of voltage values over time\"\n",
    "    def __init__(self, test=-1, text_file=None, **kwargs):\n",
    "        self.text_file = None\n",
    "        self.waveform = None\n",
    "        self.test = test\n",
    "        self.time = None\n",
    "        self.duration = None\n",
    "        self.interval = None\n",
    "        self.channel = None\n",
    "        self.amplitude = None\n",
    "        self.entropy = None\n",
    "        self.spectrogram = None\n",
    "        self.normalized = None\n",
    "\n",
    "    def trimmed(self):                          # trim off trailing zeroes due to HLT\n",
    "        return self.waveform[:self.get_duration()]\n",
    "    \n",
    "    def get_spectrogram(self):                  # calculate spectrogram of the waveform\n",
    "        if self.spectrogram is None:\n",
    "            self.spectrogram = scipy.signal.spectrogram(self.waveform, nperseg=120)\n",
    "        return self.spectrogram\n",
    "    \n",
    "    def get_duration(self):                     # calculate number of points in the waveform, excluding the trailing HLT\n",
    "        if self.duration is None:\n",
    "            index = -1                      # start checking from end\n",
    "            while self.waveform[index]==0:       # while no nonzero entries have been found\n",
    "                index -= 1                  # iterate backwards through wave\n",
    "            self.duration = index + len(self.waveform)\n",
    "        return self.duration\n",
    "\n",
    "    def get_entropy(self):                          # get information entropy value for the hit\n",
    "        if self.entropy is None:\n",
    "            dist = PMF(np.abs(self.trimmed()))      # probability distribution of voltage in wave\n",
    "            self.entropy = scipy.stats.entropy(dist,base=2) # binary entropy of the PMF\n",
    "        return self.entropy\n",
    "\n",
    "    def ID(self):\n",
    "        return self.text_file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "\n",
    "    def get_amplitude(self):                        # get amplitude value for the hit\n",
    "        if self.amplitude is None:\n",
    "            self.amplitude = 20*np.log10(np.max(np.abs(self.waveform))) + 80\n",
    "        return self.amplitude\n",
    "    \n",
    "    def read_text(self, fname=None):\n",
    "        fieldval = lambda id, s, val: s.split(\": \")[-1] if id in s else val\n",
    "        if fname is not None:\n",
    "            self.text_file = fname\n",
    "        with open(self.text_file, mode='r') as f:\n",
    "            txt = f.readlines()\n",
    "        for ind, line in enumerate(txt):            \n",
    "            if line==\"\\n\":\n",
    "                break\n",
    "            self.time = fieldval(\"TIME OF TEST:\", line, self.time)\n",
    "            self.interval = fieldval(\"SAMPLE INTERVAL (Seconds):\", line, self.interval)\n",
    "            self.channel = fieldval(\"CHANNEL NUMBER:\", line, self.channel)\n",
    "        else:\n",
    "            raise ValueError(f\"Expected blank line not found in waveform datafile {fname}\")\n",
    "        self.waveform = np.array(txt[ind+1:], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, ID=-1):\n",
    "        self.ID = ID                        # test number (or name) for dataset        \n",
    "        self.hits = []                      # initial list of datapoints\n",
    "        self.saveprops = [\"waveform\", \"spectrogram\", \"entropy\", \"time\",\n",
    "                \"amplitude\", \"channel\", \"text_file\", \"duration\", \"test\"]\n",
    "    \n",
    "    def __getattr__(self, __name: str):     # gives easy access to hit attributes\n",
    "        return np.array([hit.__getattribute__(__name) for hit in self.hits])\n",
    "\n",
    "    def __len__(self) -> int:               # return number of hits from len() function\n",
    "        return len(self.hits)\n",
    "\n",
    "    def __getitem__(self,key):              \n",
    "        if isinstance(key,int):             # for integer indices\n",
    "            return self.hits[key]           # returns hit when dataset is indexed\n",
    "        else:                               # for numpy-like boolean indexing\n",
    "            new_hits = []\n",
    "            for i in range(len(self.hits)):\n",
    "                if key[i]:\n",
    "                    new_hits.append(self.hits[i])\n",
    "            new_set = Dataset()\n",
    "            new_set.hits = new_hits     \n",
    "            return new_set            \n",
    "\n",
    "    def __add__(self,other):                # allows for datasets to be combined\n",
    "        new = Dataset()                     # create new dataset\n",
    "        new.hits = self.hits + other.hits   # set hits for new dataset\n",
    "        return new                          # return combined dataset\n",
    "\n",
    "    def read_txts(self, dir):               # read text files\n",
    "        self.hits = []\n",
    "        for file in os.listdir(dir):\n",
    "            self.hits.append(Hit(test=self.ID))\n",
    "            self.hits[-1].read_text(dir+\"\\\\\"+file)\n",
    "\n",
    "    def set_property(self, key, array):\n",
    "        for i, hit in enumerate(self.hits):\n",
    "                hit.__setattr__(key,array[i])\n",
    "\n",
    "    def normalize_data(self, offset):            \n",
    "        x = np.log(self.spectrogram+offset)\n",
    "        self.set_property(\"normalized\", (x - np.min(x))/np.std(x))\n",
    "\n",
    "    def save_npy(self, folder):\n",
    "        dir = lambda s: folder + \"\\\\\" + s\n",
    "        '''\n",
    "        arrays = {\"waves.npy\": self.waveform,\n",
    "                  \"spectrograms.npy\": self.spectrogram,\n",
    "                  \"entropy.npy\": self.entropy,\n",
    "                  \"times.npy\": self.time,\n",
    "                  \"amplitudes.npy\": self.amplitude,\n",
    "                  \"channels.npy\": self.channel,\n",
    "                  \"textfiles.npy\": self.text_file,\n",
    "                  \"duration.npy\":self.duration}'''\n",
    "        for prop in self.saveprops:\n",
    "            np.save(dir(prop+\".npy\"), self.__getattr__(prop))\n",
    "\n",
    "    def load_npy(self, folder):\n",
    "        dir = lambda s: folder + \"\\\\\" + s\n",
    "        arrays = {}\n",
    "        for f in os.listdir(folder):\n",
    "            tag = f.split(\".\")[0]\n",
    "            if tag in self.saveprops:\n",
    "                arrays[tag] = np.load(dir(f), allow_pickle=True)\n",
    "                n = arrays[tag].shape[0]\n",
    "        self.hits = [Hit() for _ in range(n)]\n",
    "        for tag, arr in arrays.items():\n",
    "            self.set_property(tag,arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data and normalize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = (Dataset(1), Dataset(2), Dataset(3), Dataset(4))\n",
    "for i in range(1):\n",
    "    T[i].read_txts(f\"Waveform{i+1}\")\n",
    "\n",
    "data = T[0] + T[1] + T[2] + T[3]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d090375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rng = np.random.rand(len(data))\n",
    "train = rng<0.8\n",
    "\n",
    "train_data = data[train]\n",
    "test_data = data[~train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pixel intensity distribution\n",
    "plt.figure(101, figsize=(7.5,5))\n",
    "histvals = np.concatenate(np.concatenate(raw_specs))\n",
    "h=plt.hist(histvals[np.isfinite(histvals)], bins=200, log=1)\n",
    "\n",
    "plt.ylabel(\"Frequency in Dataset\")\n",
    "_ = plt.xlabel(\"Spectrogram Pixel Intensity\")\n",
    "\n",
    "# Pixel intensity distribution\n",
    "plt.figure(102, figsize=(7.5,5))\n",
    "histvals = np.concatenate(np.concatenate(specs))\n",
    "h=plt.hist(histvals[np.isfinite(histvals)], bins=200, log=1)\n",
    "\n",
    "plt.ylabel(\"Frequency in Dataset\")\n",
    "_ = plt.xlabel(\"Adjusted Spectrogram Pixel Intensity\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(models.Model):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()   # initialize tensorflow model\n",
    "        self.encoder = tf.keras.Sequential([  # convolutional model which reduces image down to 50 points\n",
    "            layers.Input(shape=(61, 58, 1)),\n",
    "            layers.Conv2D(32, 3, activation='relu'),\n",
    "            layers.Conv2D(16, 3, activation='relu'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(50)\n",
    "            ])\n",
    "        self.decoder = tf.keras.Sequential([  # inverse convolutional model which regenerates original image\n",
    "            layers.Input(shape=(50)),\n",
    "            layers.Dense(49248, activation='relu'),\n",
    "            layers.Reshape((57, 54, 16)),\n",
    "            #layers.Input(shape=(57, 54, 16)),\n",
    "            layers.Conv2DTranspose(16, 3, activation='relu'),\n",
    "            layers.Conv2DTranspose(16, 3, activation='relu'), \n",
    "            layers.Conv2DTranspose(8, 3, activation='relu'),\n",
    "            layers.Conv2D(1, 3, activation='relu')\n",
    "            ])\n",
    "\n",
    "    def call(self, x):            # ensure model calls both encoder and decoder during training\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "checkpoint_path = \"Model\\\\V04\\\\cp{epoch}.ckpt\"\n",
    "\n",
    "automodel = Autoencoder()\n",
    "automodel.encoder.summary()\n",
    "automodel.decoder.summary()\n",
    "automodel.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                                  save_weights_only=True,\n",
    "                                                  verbose=1)\n",
    "\n",
    "def encode(vals):\n",
    "    '''Encode an spectrogram into the latent space and then decode it. \n",
    "        Completes task in small batches < 100 hits to avoid exhausting GPU memory.'''\n",
    "    encoded_space = []\n",
    "    decoded_space = []\n",
    "    for i in range(0,len(vals), 100):\n",
    "        j = min(len(vals), i+100)\n",
    "        print(i, end=\" \")\n",
    "        #time.sleep(0.3)\n",
    "        encoded_space.append(automodel.encoder(vals[i:j,:,:]))\n",
    "        decoded_space.append(automodel.decoder(encoded_space[-1]))\n",
    "\n",
    "    return np.concatenate(encoded_space), np.concatenate(decoded_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weight_file = \"cp9.ckpt\"\n",
    "weight_file = \"v2cp10.ckpt\"\n",
    "\n",
    "if weight_file is None:         # train model and plot progress\n",
    "    history = automodel.fit(x=train_data.normalized, y=train_data.normalized, epochs=20, \n",
    "                validation_data=(test_data.normalized, test_data.normalized), callbacks=cp_callback)\n",
    "                \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(); plt.show()\n",
    "else:                           # load model from file\n",
    "    automodel.load_weights(weight_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, regenerated = encode(test_data.normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "flat = reducer.fit_transform(latent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Space Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent(flat, color, label, savefile=None, show=True):\n",
    "    zlim = [np.quantile(z,.995), np.quantile(z,.995)]\n",
    "    cmap = mpl.cm.viridis\n",
    "    f0 = [x for _, x in sorted(zip(s, flat[:,0]))]\n",
    "    f1 = [x for _, x in sorted(zip(s, flat[:,1]))]\n",
    "    z2 = [x for _, x in sorted(zip(s, z))]\n",
    "    \n",
    "    #plt.figure(100, figsize=(13,8))\n",
    "    plt.scatter(f0, f1, s=7e4/len(f0), c=z2, cmap=cmap, vmin=zlim[0]+1, vmax=zlim[1]+2, alpha=1)\n",
    "    plt.colorbar(label=label)\n",
    "    if savefile is not None:\n",
    "        plt.savefig(savefile, dpi=300)\n",
    "    if show:\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Figures\\\\V04_\"\n",
    "save_figs = True\n",
    "fig = lambda ID: f\"{prefix}{ID}.png\" if save_figs else None\n",
    "\n",
    "plot_latent(flat, len_val/5e3, \"Duration (ms)\", fig(1))\n",
    "plot_latent(flat, enta, \"Entropy\",fig(2))\n",
    "plot_latent(flat, sns, \"Channel #\",fig(3))\n",
    "plot_latent(flat, tst, \"Specimen #\",fig(4))\n",
    "plot_latent(flat, amps[rng>0.8], \"Amplitude (dB)\",fig(5))\n",
    "\n",
    "plot_latent(flat, enta, \"Entropy\", show=False)\n",
    "xlim = plt.xlim(); ylim = plt.ylim()\n",
    "box = [[-5,5],\n",
    "       [ -6,  2.5]]\n",
    "plt.plot(box[0]+box[0][::-1]+[box[0][0]], [box[1][0]]*2+[box[1][1]]*2+[box[1][0]])\n",
    "plt.xlim(xlim); plt.ylim(ylim)\n",
    "if save_figs:\n",
    "       plt.savefig(fig(6), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting of High-Entropy Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = np.array([box[0][0]<flat[i,0]<box[0][1] and box[1][0]<flat[i,1]<box[1][1] for i in range(flat.shape[0])])\n",
    "reducer = umap.UMAP()\n",
    "flat_cluster = reducer.fit_transform(latent[cluster,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(flat_cluster,np.array(enta)[cluster],\"Entropy\", fig(7))\n",
    "plot_latent(flat_cluster,np.array(sns)[cluster],\"Channel #\", fig(8))\n",
    "plot_latent(flat_cluster,np.array(amps)[rng>0.8][cluster],\"Amplitude (dB)\", fig(9))\n",
    "plot_latent(flat_cluster,np.array(len_val/5e3)[cluster],\"Duration (ms)\", fig(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mx = np.quantile(test_data, 0.999); print(mx)\n",
    "for i in range(0, sp_val.shape[0], 201):\n",
    "        fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "        print(i)\n",
    "        ax[0].imshow(sp_val[i], origin=\"lower\", vmin=0, vmax=mx)\n",
    "        ax[1].imshow(b[i], origin=\"lower\", vmin=0, vmax=mx)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea8238db6dc1fe8cf6fe83219457bde9bcbcde2053d40c59d2e78211d10c5fee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
