{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "138316ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, umap, math\n",
    "import scipy.stats, scipy.signal\n",
    "import numpy as np\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import layers, models\n",
    "import tensorflow as tf\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Entropy Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IQR = lambda X: np.quantile(X,.75) - np.quantile(X,.25)     # interquartile range\n",
    "outlier = lambda X: X>(np.quantile(X,.75) +IQR(X))          # boolean outlier identification\n",
    "\n",
    "def PMF(waveform):                  # probability mass function of waveform\n",
    "    hist = histogram(waveform)      # histogram of waveform\n",
    "    return hist / sum(hist)         # normalize to sum of 1\n",
    "    \n",
    "def histogram(wave):\n",
    "    \"Returns a histogram of values with bin widths determined by the Freedman-Diaconis rule.\"\n",
    "    n = wave.shape[0]                               # number of points in time series\n",
    "    data_range = np.max(wave) - np.min(wave)        # range of voltage values\n",
    "    bin_width = 2 * IQR(wave) / (n ** (1 / 3))      # width of each bin\n",
    "    num_bins = math.ceil(data_range / bin_width)    # number of bins\n",
    "    edges = bin_width*np.arange(num_bins+1)         # bin boundaries\n",
    "    # return number of points in each bin\n",
    "    return np.array([np.count_nonzero((edges[i]<=wave) & (wave<edges[i+1])) for i in range(num_bins)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hit Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hit:\n",
    "    \"A waveform with a fixed number of voltage values over time, and associated metadata and calculated values.\"\n",
    "    def __init__(self, test=-1):\n",
    "        self.text_file = None                   # .txt datafile containing waveform\n",
    "        self.waveform = None                    # array containing the waveform voltage data\n",
    "        self.test = test                        # test/data grouping number or identifier\n",
    "        self.time = None                        # time at which the hit occured during the test, seconds\n",
    "        self.duration = None                    # number of data points for which the hit continues before the HLT is triggered\n",
    "        self.interval = None                    # sampling interval of the AE sensor\n",
    "        self.channel = None                     # channel of the AE sensor\n",
    "        self.amplitude = None                   # approximate maximum amplitude of the hit\n",
    "        self.entropy = None                     # information entropy of voltage PMF for the hit\n",
    "        self.spectrogram = None                 # spectrogram of the hit\n",
    "        self.normalized = None                  # log-normalized spectrogram\n",
    "        self.latent = None                      # latent space vector from autoencoder model\n",
    "        self.flat = None                        # flattened latente space generated with umap\n",
    "        self.decoded = None                     # regenerated spectrogram from autoencoder model\n",
    "\n",
    "    def calculate_all(self):\n",
    "        \"Calculate all relevant values for the dataset\"\n",
    "        self.get_duration()                     # determine the duration of each hit\n",
    "        self.get_spectrogram()                  # get the spectrogram for the hit\n",
    "        self.get_entropy()                      # get the entropy value of the hit\n",
    "        self.get_amplitude()                    # get the amplitude of the hit\n",
    "    \n",
    "    def get_spectrogram(self) -> np.ndarray:\n",
    "        \"Calculate a spectrogram of the waveform using scipy function\"\n",
    "        if self.spectrogram is None:\n",
    "            self.spectrogram = scipy.signal.spectrogram(self.waveform, nperseg=120)[2]\n",
    "        return self.spectrogram\n",
    "    \n",
    "    def get_duration(self) -> int:\n",
    "        \"Calculate the number of points in the waveform, excluding the trailing HLT\"\n",
    "        if self.duration is None:\n",
    "            index = -1                      # start checking from end\n",
    "            while self.waveform[index]==0:  # while no nonzero entries have been found\n",
    "                index -= 1                  # iterate backwards through wave\n",
    "            self.duration = index + len(self.waveform)\n",
    "        return self.duration\n",
    "\n",
    "    def get_entropy(self) -> float:\n",
    "        \"return information entropy value for hit\"\n",
    "        if self.entropy is None:\n",
    "            dist = PMF(np.abs(self.trimmed()))      # probability distribution of voltage in wave\n",
    "            self.entropy = scipy.stats.entropy(dist,base=2) # binary entropy of the PMF\n",
    "        return self.entropy\n",
    "        \n",
    "    def trimmed(self) -> np.ndarray: \n",
    "        \"return waveform without trailing zeroes due to HLT\"\n",
    "        return self.waveform[:self.get_duration()]  # waveform up to last nonzero point\n",
    "\n",
    "    def get_amplitude(self) -> float:\n",
    "        \"return amplitude value for the hit\"\n",
    "        if self.amplitude is None:\n",
    "            self.amplitude = 20*np.log10(np.max(np.abs(self.waveform))) + 80\n",
    "        return self.amplitude\n",
    "    \n",
    "    def read_text(self, fname=None):\n",
    "        \"read voltage and metadata from .txt file\"\n",
    "        fieldval = lambda id, s, val: s.split(\": \")[-1] if id in s else val\n",
    "        if fname is not None:\n",
    "            self.text_file = fname\n",
    "        with open(self.text_file, mode='r') as f:\n",
    "            txt = f.readlines()\n",
    "        for ind, line in enumerate(txt):            \n",
    "            if line==\"\\n\":\n",
    "                break\n",
    "            self.time = fieldval(\"TIME OF TEST:\", line, self.time)\n",
    "            self.interval = fieldval(\"SAMPLE INTERVAL (Seconds):\", line, self.interval)\n",
    "            self.channel = fieldval(\"CHANNEL NUMBER:\", line, self.channel)\n",
    "        else:\n",
    "            raise ValueError(f\"Expected blank line not found in waveform datafile {fname}\")\n",
    "        self.waveform = np.array(txt[ind+1:], dtype=float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    \"A dataset comprising a group of AE hits from one or more test setups.\"\n",
    "    def __init__(self, ID=-1):\n",
    "        self.ID = ID                        # test number (or name) for dataset        \n",
    "        self.hits = []                      # initial list of datapoints\n",
    "        self.saveprops = [\"waveform\", \"spectrogram\", \"entropy\", \"time\", \"amplitude\", \"channel\",     # list of attributes to save as arrays\n",
    "                          \"text_file\", \"duration\", \"test\", \"normalized\", \"latent\", \"flat\"]\n",
    "    \n",
    "    def __getattr__(self, __name: str):\n",
    "        \"gives easy access to individual hit attributes and methods through dot-notation overloading\"\n",
    "        class FunctionArray:\n",
    "            '''Custom class allowing every function in an array to be called simultaneously \n",
    "            with the same set of arguments, returning an array of the function return values'''\n",
    "            def __init__(self, array):                  # passes array of functions to object\n",
    "                self.array = array\n",
    "            def __call__(self, *args, **kwargs):        # call all functions in array and return the results\n",
    "                return np.array([f(*args, **kwargs) for f in self.array])\n",
    "\n",
    "        arr = np.array([hit.__getattribute__(__name) for hit in self.hits])         # array of hit attributes or methods\n",
    "        return FunctionArray(arr) if all([callable(attr) for attr in arr]) else arr # if array is made of methods, convert to custom class\n",
    "\n",
    "    def __len__(self) -> int:               \n",
    "        return len(self.hits)               # return number of hits from len() function\n",
    "\n",
    "    def __getitem__(self,key):\n",
    "        \"overloads indexing using square brackets to access hits and provide logical indexing\"\n",
    "        if isinstance(key,int):             # if index provided is integer\n",
    "            return self.hits[key]           # returns hit when dataset is indexed\n",
    "        else:                               # for numpy-like logical indexing\n",
    "            new_hits = []                   # list of hits to be returned\n",
    "            for i in range(len(self.hits)): # iterate over hits\n",
    "                if key[i]:                  # check logical index of hit \n",
    "                    new_hits.append(self.hits[i])\n",
    "            new_set = Dataset()             # create new dataset\n",
    "            new_set.hits = new_hits         # set list of hits in new dataset\n",
    "            return new_set                  # return new dataset\n",
    "\n",
    "    def __add__(self,other):                # allows for datasets combination through '+' operator overloading\n",
    "        new = Dataset()                     # create new dataset\n",
    "        new.hits = self.hits + other.hits   # set hits for new dataset\n",
    "        return new                          # return combined dataset\n",
    "\n",
    "    def read_texts(self, dir):\n",
    "        \"read text files from provided directory\"\n",
    "        self.hits = []                              # clear any existing hits\n",
    "        for file in os.listdir(dir):                # iterate through files\n",
    "            self.hits.append(Hit(test=self.ID))     # add new hit\n",
    "            self.hits[-1].read_text(dir+\"\\\\\"+file)  # read file contents for hit\n",
    "\n",
    "    def set_property(self, key, array):\n",
    "        \"Set an attribute for each hit from a provided array\"\n",
    "        for i, hit in enumerate(self.hits):         # iterate through hits\n",
    "                hit.__setattr__(key,array[i])       # set atribute for each\n",
    "\n",
    "    def normalize_data(self, offset: float=1e-8):\n",
    "        \"log-normalize the dataset, with an offset to prevent zero-values raising errors\"   \n",
    "        x = np.log(self.spectrogram+offset)         # natural log of each spectrogram data point\n",
    "        self.set_property(\"normalized\", (x - np.min(x))/np.std(x))      # normalize to min=0 and stdev=1\n",
    "\n",
    "    def save_npy(self, dir):\n",
    "        \"save the dataset as a folder of numpy arrays\"         \n",
    "        for prop in self.saveprops:                 # iterate through property list and save each item\n",
    "            np.save(f\"{dir}\\\\{prop}.npy\", self.__getattr__(prop))\n",
    "\n",
    "    def load_npy(self, dir):             \n",
    "        \"load the dataset from a folder of numpy arrays\"\n",
    "        arrays = {}\n",
    "        for f in os.listdir(dir):                   # iterate through directory files\n",
    "            tag = f.split(\".\")[0]                   # attribute identifier\n",
    "            if tag in self.saveprops:               # check if file matches property for hit\n",
    "                arrays[tag] = np.load(f\"{dir}\\\\{f}.npy\", allow_pickle=True)\n",
    "                n = arrays[tag].shape[0]\n",
    "        self.hits = [Hit() for _ in range(n)]       # create list of empty hits\n",
    "        for tag, arr in arrays.items():             # iterate through properties\n",
    "            self.set_property(tag,arr)              # set property for each hit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Data and normalize values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_npy = True\n",
    "if not load_npy:\n",
    "    trials = (Dataset(1), Dataset(2), Dataset(3), Dataset(4))   # tests #1-4\n",
    "    for i in range(len(trials)):\n",
    "        trials[i].read_texts(f\"Waveform{i+1}\")              # read waveform files\n",
    "        trials[i].calculate_all()                           # find spectrograms, entropy, etc\n",
    "    data = trials[0] + trials[1] + trials[2] + trials[3]    # combine datasets from each test sample\n",
    "    data.normalize_data()                                   # log-normalize the dataset\n",
    "    data.save_npy(\"Datasets\\\\V05\")                          # save the dataset to a folder\n",
    "else:\n",
    "    data = Dataset(); data.load_npy(\"Datasets\\\\V04\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Test/Train Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d090375c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "rng = np.random.rand(len(data))\n",
    "train = rng<0.8\n",
    "\n",
    "train_data = data[train]\n",
    "test_data = data[~train]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building an Autoencoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(models.Model):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()   # initialize tensorflow model\n",
    "        self.encoder = tf.keras.Sequential([  # convolutional model which reduces image down to 50 points\n",
    "            layers.Input(shape=(61, 58, 1)),\n",
    "            layers.Conv2D(32, 3, activation='relu'),\n",
    "            layers.Conv2D(16, 3, activation='relu'),\n",
    "            layers.Flatten(),\n",
    "            layers.Dense(50)\n",
    "            ])\n",
    "        self.decoder = tf.keras.Sequential([  # inverse convolutional model which regenerates original image\n",
    "            layers.Input(shape=(50)),\n",
    "            layers.Dense(49248, activation='relu'),\n",
    "            layers.Reshape((57, 54, 16)),\n",
    "            #layers.Input(shape=(57, 54, 16)),\n",
    "            layers.Conv2DTranspose(16, 3, activation='relu'),\n",
    "            layers.Conv2DTranspose(16, 3, activation='relu'), \n",
    "            layers.Conv2DTranspose(8, 3, activation='relu'),\n",
    "            layers.Conv2D(1, 3, activation='relu')\n",
    "            ])\n",
    "\n",
    "    def call(self, x):            # ensure model calls both encoder and decoder during training\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "checkpoint_path = \"Model\\\\V04\\\\cp{epoch}.ckpt\"\n",
    "\n",
    "automodel = Autoencoder()           # create autoencoder model\n",
    "automodel.compile(optimizer='adam', loss=MeanSquaredError()) # compile autoencoder model\n",
    "\n",
    "# Create a callback that saves the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path, save_weights_only=True)\n",
    "\n",
    "def encode(dataset):\n",
    "    '''Encode an spectrogram into the latent space and then decode it. \n",
    "        Completes task in small batches < 100 hits to avoid exhausting GPU memory.'''\n",
    "    encoded = []; decoded = []\n",
    "    for i in range(0,len(dataset), 100):\n",
    "        j = min(len(dataset), i+100)\n",
    "        encoded.append(automodel.encoder(dataset.normalized[i:j,:,:]))    \n",
    "        decoded.append(automodel.decoder(encoded[-1]))\n",
    "    test_data.set_property(\"latent\", np.concatenate(encoded))\n",
    "    test_data.set_property(\"decoded\", np.concatenate(decoded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\david\\OneDrive - Drexel University\\Documents\\GitHub\\MEMT680-Project\\Simplified_main.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/OneDrive%20-%20Drexel%20University/Documents/GitHub/MEMT680-Project/Simplified_main.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m weight_file \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/OneDrive%20-%20Drexel%20University/Documents/GitHub/MEMT680-Project/Simplified_main.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mif\u001b[39;00m weight_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:         \u001b[39m# train model and plot progress\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/david/OneDrive%20-%20Drexel%20University/Documents/GitHub/MEMT680-Project/Simplified_main.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     history \u001b[39m=\u001b[39m automodel\u001b[39m.\u001b[39;49mfit(x\u001b[39m=\u001b[39;49mtrain_data\u001b[39m.\u001b[39;49mnormalized, y\u001b[39m=\u001b[39;49mtrain_data\u001b[39m.\u001b[39;49mnormalized, epochs\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/OneDrive%20-%20Drexel%20University/Documents/GitHub/MEMT680-Project/Simplified_main.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                 validation_data\u001b[39m=\u001b[39;49m(test_data\u001b[39m.\u001b[39;49mnormalized, test_data\u001b[39m.\u001b[39;49mnormalized), callbacks\u001b[39m=\u001b[39;49mcp_callback)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/OneDrive%20-%20Drexel%20University/Documents/GitHub/MEMT680-Project/Simplified_main.ipynb#X26sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m], label\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/david/OneDrive%20-%20Drexel%20University/Documents/GitHub/MEMT680-Project/Simplified_main.ipynb#X26sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     plt\u001b[39m.\u001b[39mplot(history\u001b[39m.\u001b[39mhistory[\u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m], label \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mval_loss\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\david\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "#weight_file = \"v2cp10.ckpt\"\n",
    "weight_file = None\n",
    "if weight_file is None:         # train model and plot progress\n",
    "    history = automodel.fit(x=train_data.normalized, y=train_data.normalized, epochs=5, \n",
    "                validation_data=(test_data.normalized, test_data.normalized), callbacks=cp_callback)\n",
    "                \n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label = 'val_loss')\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.legend(); plt.show()\n",
    "else:                           # load model from file\n",
    "    automodel.load_weights(weight_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent, recoded = encode(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Latent Space Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP()\n",
    "flat = reducer.fit_transform(latent)\n",
    "test_data.set_property(\"flat\", flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffle = lambda X: np.array([xi for _, xi in sorted(zip(rng, X))])\n",
    "\n",
    "def plot_latent(flat, color, label, savefile=None, show=True):\n",
    "    \"Plots flattened latent space points, along with a specified color scale\"\n",
    "    vlim = [np.quantile(color,.01), np.quantile(color,.99)]         # colorbar limits for scatterplot\n",
    "    (fx, fy) = (shuffle(flat[:,0]), shuffle(flat[:,1]))             # shuffled x-y coordinates to plot (so plot order is random)\n",
    "    c = shuffle(color)                                              # shuffled color values\n",
    "    plt.scatter(fx, fy, s=7e4/len(fx), c=c, cmap=mpl.cm.viridis, vmin=vlim[0], vmax=vlim[1])        # create scatterplot\n",
    "    plt.colorbar(label=label)\n",
    "    if savefile is not None:                                        # save figure\n",
    "        plt.savefig(savefile, dpi=300)\n",
    "    if show:                                                        # show figure\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"Figures\\\\V04_\"\n",
    "save_figs = True\n",
    "fig = lambda ID: f\"{prefix}{ID}.png\" if save_figs else None\n",
    "\n",
    "plot_latent(flat, test_data.duration*test_data.interval*1e3, \"Duration (ms)\", fig(1))\n",
    "plot_latent(flat, test_data.entropy, \"Entropy\",fig(2))\n",
    "plot_latent(flat, test_data.channel, \"Channel #\",fig(3))\n",
    "plot_latent(flat, test_data.test, \"Specimen #\",fig(4))\n",
    "plot_latent(flat, test_data.amplitude, \"Amplitude (dB)\",fig(5))\n",
    "\n",
    "plot_latent(flat, test_data.entropy, \"Entropy\", show=False)\n",
    "xlim = plt.xlim(); ylim = plt.ylim()\n",
    "box = [[-5,5],\n",
    "       [ -6,  2.5]]\n",
    "plt.plot(box[0]+box[0][::-1]+[box[0][0]], [box[1][0]]*2+[box[1][1]]*2+[box[1][0]])\n",
    "plt.xlim(xlim); plt.ylim(ylim)\n",
    "if save_figs:\n",
    "       plt.savefig(fig(6), dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Plotting of High-Entropy Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = np.array([box[0][0]<flat[i,0]<box[0][1] and box[1][0]<flat[i,1]<box[1][1] for i in range(flat.shape[0])])\n",
    "reducer = umap.UMAP()\n",
    "flat_cluster = reducer.fit_transform(latent[cluster,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent(flat_cluster,test_data.entropy[cluster],\"Entropy\", fig(7))\n",
    "plot_latent(flat_cluster,test_data.channel[cluster],\"Channel #\", fig(8))\n",
    "plot_latent(flat_cluster,test_data.amplitude[cluster],\"Amplitude (dB)\", fig(9))\n",
    "plot_latent(flat_cluster,test_data.duration[cluster]*test_data.interval[cluster],\"Duration (ms)\", fig(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b8d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "vmax = np.quantile(test_data, 0.99)\n",
    "for i in range(0, sp_val.shape[0], 5000):\n",
    "        fig, ax = plt.subplots(1,2, figsize=(10,10))\n",
    "        ax[0].imshow(test_data.normalized, origin=\"lower\", vmin=0, vmax=vmax)\n",
    "        ax[1].imshow(rec, origin=\"lower\", vmin=0, vmax=vmax)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ea8238db6dc1fe8cf6fe83219457bde9bcbcde2053d40c59d2e78211d10c5fee"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
